{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2637a65b",
   "metadata": {},
   "source": [
    "# **Second assignment: Decision trees** #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c09071",
   "metadata": {},
   "source": [
    "**Assigment done by Marta Longo, Lucas Oliveira, Sara TÃ¡boas for Artificial Inteligence Class**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9255dc82",
   "metadata": {},
   "source": [
    "# **Table of Contents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1db0f1",
   "metadata": {},
   "source": [
    "<!-- Hello World -->\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Libraries](#Libraries)\n",
    "3. [ID3 Algorithm](#ID3-Algorithm)\n",
    "    - [Implementation of Class Tree and Tree Node](#Implementation-of-Class-Tree-and-Tree-Node)\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Introduction](#Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b2a65",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187e378",
   "metadata": {},
   "source": [
    "The purpose of this assigment is to write a program that learns a decision tree from three training datasets that were given to us using the ID3 algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1630366",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d2136e",
   "metadata": {},
   "source": [
    "# ID3 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43e91e0",
   "metadata": {},
   "source": [
    "`The ID3 algorithm is a popular method used to create decision trees, which are a type of predictive model used in machine learning and statistics. ID3 works by selecting the attribute that provides the highest information gain to split the data, aiming to reduce entropy (impurity) in the dataset. The process is repeated recursively for each subset, with the attribute that best separates the data chosen at each step, forming a tree structure where each node represents a decision based on an attribute. The recursion stops when all instances in a subset belong to the same class, no more attributes are available, or there are no more instances to split. Here are the key elements:`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7a506c",
   "metadata": {},
   "source": [
    "## **Implementation of Class Tree Node and Tree**\n",
    "[go back to the top](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b5495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, feature_index=None, value=None, label=None, counter=0, children=None, feature_name = None):\n",
    "        self.feature_index = feature_index  # Index of the feature used for splitting\n",
    "        self.value = value  # Value used for splitting (if categorical)\n",
    "        self.label = label  # Class label (if leaf node)\n",
    "        self.counter = counter  # Counter for the number of examples\n",
    "        self.children = children  # Dictionary of children nodes\n",
    "        self.feature_name = feature_name  #the feature that was selected\n",
    "      \n",
    "    \n",
    "    def __str__(self):\n",
    "        return (\"Index: \" + str(self.feature_index) + \"\\nValue: \" + str(self.value) + \"\\nLabel:\" + str(self.label) + \"\\nChildren:\" + str(self.children))\n",
    "    \n",
    " \n",
    "    \n",
    "    def print_tree(self, indent=\"\", instance_counter=None):\n",
    "        if instance_counter is None:\n",
    "            instance_counter = [0]  # Initialize the counter as a list to pass by reference\n",
    "\n",
    "        instance_counter[0] += 1  # Increment the counter for each node visited\n",
    "        if self.label is not None:\n",
    "            return f\"{self.label} ({instance_counter})\"\n",
    "        \n",
    "        result = []\n",
    "        if isinstance(self.value, list):\n",
    "            result.append(f\"\\n{indent}<{self.feature_name}>\")\n",
    "            for v in self.value:\n",
    "                child = self.children[v]\n",
    "                result.append(f\"\\n{indent}  {v}: {child.print_tree(indent + '    ', instance_counter)}\")\n",
    "        else:\n",
    "            result.append(f\"{indent}<{self.feature_name}> <= {self.value}\")\n",
    "            result.append(f\"\\n{indent}  {self.children['left'].print_tree(indent + '    ', instance_counter)}\")\n",
    "            result.append(f\"\\n{indent}<{self.feature_name}> > {self.value}\")\n",
    "            result.append(f\"\\n{indent}  {self.children['right'].print_tree(indent + '    ', instance_counter)}\")\n",
    "        \n",
    "        return \"\\n\".join(result)\n",
    "    \n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def entropy(self, labels):\n",
    "        # Calculate entropy (measure of impurity)\n",
    "        from collections import Counter\n",
    "        counts = Counter(labels)\n",
    "        probabilities = [count / len(labels) for count in counts.values()]\n",
    "        entropy = 0\n",
    "        for p in probabilities:\n",
    "            log_value = math.log2(p)\n",
    "            entropy += -p * log_value\n",
    "        return entropy\n",
    "\n",
    "    def cat_information_gain(self, data, feature_index, values):\n",
    "        labels = {}\n",
    "        for v in values:\n",
    "            labels[v] = []\n",
    "        for row in data:\n",
    "            for v in values:\n",
    "                if row[feature_index] == v:\n",
    "                    labels[v].append(row[-1])\n",
    "        result = self.entropy([label for row in data for label in row[-1]])\n",
    "        for v in values:\n",
    "            \n",
    "            result -= self.entropy(labels[v]) * len(labels[v])/len(data)\n",
    "        return result\n",
    "\n",
    "    def information_gain(self, data, feature_index, threshold):\n",
    "        # Calculate information gain for a split\n",
    "        left_labels, right_labels = [], []\n",
    "        for row in data:\n",
    "            # print(feature_index, row[feature_index], threshold)           #visualize feature index being analyzed\n",
    "            if row[feature_index] <= threshold:\n",
    "                left_labels.append(row[-1])\n",
    "            else:\n",
    "                right_labels.append(row[-1])\n",
    "        parent_entropy = self.entropy([label for row in data for label in row[-1]])\n",
    "        left_entropy = self.entropy(left_labels)\n",
    "        right_entropy = self.entropy(right_labels)\n",
    "        weight = len(left_labels) / len(data)\n",
    "        return parent_entropy - (weight * left_entropy + (1 - weight) * right_entropy)\n",
    "\n",
    "    def fit(self, data, feature_name):\n",
    "        # Train the decision tree\n",
    "        self.feature_name = feature_name\n",
    "        self.root = self._build_tree(data, 0)\n",
    "    \n",
    "    def _build_tree(self, data, depth):\n",
    "        # Recursive function to build the tree\n",
    "        if depth >= self.max_depth or len(data) < self.min_samples_split or len(set([row[-1] for row in data])) == 1:\n",
    "        # Stopping conditions: reach max depth, min samples, or pure data\n",
    "            aux = majority_vote(data)\n",
    "            return TreeNode(label=aux)  # Assign majority class label\n",
    "\n",
    "        best_gain, best_feature_index, best_threshold = 0, None, None\n",
    "        evaluated_data_type = None\n",
    "        best_values = []\n",
    "        # print(\"Part 1\")\n",
    "        for feature_index in range(len(data[0]) - 1):\n",
    "            # print(\"Part 2\")\n",
    "            if type(data[1][feature_index]) != str:\n",
    "                # print(\"NUM\")\n",
    "                # Find the best split among all features\n",
    "                values = set([row[feature_index] for row in data])\n",
    "                for value in values:\n",
    "                    # print(\"Part 3\")\n",
    "                    gain = self.information_gain(data.copy(), feature_index, value)\n",
    "                    if gain > best_gain:\n",
    "                        # print(\"Part 4\")\n",
    "                        best_values = values.copy()\n",
    "                        best_gain, best_feature_index, best_threshold = gain, feature_index, value\n",
    "                        evaluated_data_type = \"num\"\n",
    "                        # print(\"Best gain (num):\", feature_index)\n",
    "            else:\n",
    "                # print(\"CAT\")\n",
    "                # print(\"Part 3\")\n",
    "                values = [item for item in set([row[feature_index] for row in data])]\n",
    "                gain = self.cat_information_gain(data.copy(), feature_index, values)\n",
    "                if gain > best_gain:\n",
    "                    # print(\"Part 4\")\n",
    "                    best_values = values.copy()\n",
    "                    best_gain, best_feature_index, best_threshold = gain, feature_index, 0\n",
    "                    evaluated_data_type = \"cat\"\n",
    "                    # print(\"Best gain (cat):\", feature_index)\n",
    "                    \n",
    "        # print(\"USED:\", best_feature_index)\n",
    "        sets_of_data = {}\n",
    "        children = {}\n",
    "        if evaluated_data_type == \"cat\":\n",
    "            for v in best_values:\n",
    "                sets_of_data[v] = []\n",
    "            for row in data:\n",
    "                for v in best_values:\n",
    "                    # print(row[best_feature_index], row[best_feature_index] == v)\n",
    "                    if row[best_feature_index] == v:\n",
    "                        sets_of_data[v].append(row)\n",
    "            for v in best_values:\n",
    "                # print(\"Data:\", v)\n",
    "                # print(sets_of_data[v])\n",
    "                children[v] = self._build_tree(sets_of_data[v], depth + 1)\n",
    "                \n",
    "            return TreeNode(\n",
    "                feature_index=best_feature_index,\n",
    "                value=best_values,\n",
    "                children=children,\n",
    "                feature_name=self.feature_name[0][best_feature_index]\n",
    "                \n",
    "            )\n",
    "        else:\n",
    "            sets_of_data[\"left\"] = []\n",
    "            sets_of_data[\"right\"] = []\n",
    "            for row in data:\n",
    "                if row[best_feature_index] <= best_threshold:\n",
    "                    sets_of_data[\"left\"].append(row)\n",
    "                else:\n",
    "                    sets_of_data[\"right\"].append(row)\n",
    "            children[\"left\"] = self._build_tree(sets_of_data[\"left\"], depth + 1)\n",
    "            children[\"right\"] = self._build_tree(sets_of_data[\"right\"], depth + 1)\n",
    "            return TreeNode(\n",
    "                feature_index=best_feature_index,\n",
    "                value=best_threshold,\n",
    "                children=children,\n",
    "                feature_name=self.feature_name[0][best_feature_index]\n",
    "            )\n",
    "\n",
    "    def predict(self, datapoint):\n",
    "        # Predict class label for a new data point\n",
    "        node = self.root\n",
    "        counter = 0\n",
    "        while node.label is None:\n",
    "            if type(node.value) is list:\n",
    "                for value in node.value:\n",
    "                    if datapoint[node.feature_index] == value:\n",
    "                        node = node.children[str(value)]\n",
    "                        counter += 1\n",
    "                        break\n",
    "            else:\n",
    "                if datapoint[node.feature_index] <= node.value:\n",
    "                    node = node.children[\"left\"]\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    node = node.children[\"right\"]\n",
    "                    counter += 1\n",
    "        return node.label\n",
    "    \n",
    "\n",
    "    def print_tree(self):\n",
    "        if self.root:\n",
    "            return self.root.print_tree()\n",
    "        \n",
    "        return \"Tree is empty.\"\n",
    "\n",
    "\n",
    "\n",
    "# Helper function to find the majority class label\n",
    "def majority_vote(data):\n",
    "  labels = [row[-1] for row in data]\n",
    "  from collections import Counter\n",
    "  counts = Counter(labels)\n",
    "#   print(counts.most_common(1))\n",
    "  return counts.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30309d14",
   "metadata": {},
   "source": [
    "**TreeNode Class Attributes**\n",
    "- **feature_index**: This is the index of the feature used to split the data at this node. It identifies which feature (among the input features) is being used to decide the split at this node. If this node is a leaf node (i.e., it does not split further), feature_index will be None.\n",
    "\n",
    "\n",
    "- **value**: This is the value used for splitting the data at this node if the feature is continuous. For categorical features, this could be the category value. It represents the threshold or category against which the feature values are compared. If this node is a leaf node, value will be None.\n",
    "\n",
    "- **label**: This is the class label that the node predicts if it is a leaf node. For non-leaf nodes, this will be None.\n",
    "\n",
    "- **counter**: This represents the count of examples that reach this node during the tree building process.\n",
    "\n",
    "- **children**: This is a dictionary that stores the children nodes of the current node. Each key in the dictionary represents a particular value or range of values of the feature associated with the current node. The values associated with these keys are the child nodes resulting from splitting the data based on the corresponding feature value. For categorical features, the keys represent distinct categories, while for continuous features, the keys can represent different partitions of the feature space.\n",
    "\n",
    "- **feature_name**: This is the name/label of the feature associated with the current node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70518561",
   "metadata": {},
   "source": [
    "**Decision Tree Class Attributes**\n",
    "- **max_depth**: This specifies the maximum depth that the tree is allowed to grow to. If None, the tree will grow until all leaves are pure or until there are fewer than min_samples_split samples in the leaves. Limiting the depth helps in controlling overfitting.\n",
    "\n",
    "- **min_samples_split**: This defines the minimum number of samples required to split an internal node. This parameter prevents the tree from splitting too much and helps in controlling overfitting. If the number of samples at a node is less than min_samples_split, the node will not be split further.\n",
    "\n",
    "- **root**: This is the root node of the decision tree. It is the starting point for making predictions. Initially, this is None until the tree is fitted with data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e989e",
   "metadata": {},
   "source": [
    "## **Entropy and Information Gain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df74885",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def entropy(self, labels):\n",
    "        # Calculate entropy (measure of impurity)\n",
    "        from collections import Counter\n",
    "        counts = Counter(labels)\n",
    "        probabilities = [count / len(labels) for count in counts.values()]\n",
    "        print(\"prob = \", probabilities) \n",
    "        # from math import log2\n",
    "        entropy = 0\n",
    "        for p in probabilities:\n",
    "            log_value = math.log2(p)\n",
    "            entropy += -p * log_value\n",
    "        return entropy\n",
    "\n",
    "\n",
    "def cat_information_gain(self, data, feature_index, values):\n",
    "    labels = {}\n",
    "    for v in values:\n",
    "        labels[v] = []\n",
    "    for row in data:\n",
    "        for v in values:\n",
    "            if row[feature_index] == v:\n",
    "                labels[v].append(row[-1])\n",
    "    result = self.entropy([label for row in data for label in row[-1]])\n",
    "    for v in values:\n",
    "        \n",
    "        result -= self.entropy(labels[v]) * len(labels[v])/len(data)\n",
    "    return result\n",
    "\n",
    "    \n",
    "def information_gain(self, data, feature_index, threshold):\n",
    "    # Calculate information gain for a split\n",
    "    left_labels, right_labels = [], []\n",
    "    for row in data:\n",
    "        # print(feature_index, row[feature_index], threshold)           \n",
    "        if row[feature_index] <= threshold:\n",
    "            left_labels.append(row[-1])\n",
    "        else:\n",
    "            right_labels.append(row[-1])\n",
    "    parent_entropy = self.entropy([label for row in data for label in row[-1]])\n",
    "    left_entropy = self.entropy(left_labels)\n",
    "    right_entropy = self.entropy(right_labels)\n",
    "    weight = len(left_labels) / len(data)\n",
    "    return parent_entropy - (weight * left_entropy + (1 - weight) * right_entropy)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c61b221",
   "metadata": {},
   "source": [
    "### 1. **entropy(self, labels):** To calculate the entropy of a set of labels. Entropy is a measure of the impurity or randomness in the dataset.\n",
    "\n",
    "#### Functionality\n",
    "- **Count occurrences**: Uses Counter from the collections module to count the occurrences of each unique label.\n",
    "- **Calculate probabilities**: Computes the probability of each label by dividing the count of each label by the total number of labels.\n",
    "- **Calculate entropy**: Uses the formula $$\\text{Entropy} = -\\sum(p \\log_2 p)$$  where \\( p \\) is the probability of a label. This is summed over all unique labels.\n",
    "- **Return entropy**: Returns the calculated entropy value.\n",
    "\n",
    "### **2. cat_information_gain(self, data, feature_index, values):** To calculate the information gain for a categorical feature. Information gain measures the reduction in entropy achieved by splitting the data based on a feature.\n",
    "- **Initialize labels dictionary**: Creates a dictionary labels to hold lists of class labels for each unique value of the categorical feature.\n",
    "- **Distribute labels**: Iterates over the rows in the dataset and assigns the class label (last element in the row) to the appropriate list in the labels dictionary based on the feature value.\n",
    "- **Calculate parent entropy**: Computes the entropy of the entire dataset's class labels using the entropy method.\n",
    "- **Calculate information gain**:\n",
    "  - For each unique value in the feature, calculate the entropy of the corresponding subset of labels.\n",
    "  - Adjust the parent entropy by subtracting the weighted entropy of each subset (proportional to its size).\n",
    "- **Return information gain**: Returns the resulting information gain.\n",
    "\n",
    "### **3. information_gain(self, data, feature_index, threshold):** To calculate the information gain for a numerical feature based on a given threshold. This is used to evaluate the best threshold for splitting the data.\n",
    "\n",
    "- **Initialize label lists**: Creates two lists, left_labels and right_labels, to hold class labels for the left and right subsets created by the threshold split.\n",
    "- **Distribute labels**: Iterates over the rows in the dataset and assigns the class label (last element in the row) to either left_labels or right_labels based on whether the feature value is less than or equal to the threshold.\n",
    "- **Calculate parent entropy**: Computes the entropy of the entire dataset's class labels using the entropy` method.\n",
    "- **Calculate subset entropies**: Computes the entropy of left_labels and right_labels.\n",
    "- **Calculate weighted average entropy**: Computes the weighted average of the left and right entropies based on their sizes relative to the total dataset.\n",
    "- **Return information gain**: Subtracts the weighted average entropy from the parent entropy to get the information gain.\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "- **entropy**: Calculates the entropy of a list of labels, providing a measure of impurity.\n",
    "- **cat_information_gain**: Calculates the information gain for a categorical feature, assessing how much splitting the data on this feature reduces impurity.\n",
    "- **information_gain**: Calculates the information gain for a numerical feature using a specific threshold, evaluating the effectiveness of this split in reducing impurity.\n",
    "\n",
    "These functions are essential for decision tree algorithms, helping to determine the best features and thresholds for splitting the data to achieve the most significant reduction in impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5a7b5f",
   "metadata": {},
   "source": [
    "## **Train Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823cebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def fit(self, data):\n",
    "        # Train the decision tree\n",
    "        self.root = self._build_tree(data, 0)\n",
    "    \n",
    "def _build_tree(self, data, depth):\n",
    "    # print(\"Depth:\", depth)\n",
    "    # Recursive function to build the tree\n",
    "    if depth >= self.max_depth or len(data) < self.min_samples_split or len(set([row[-1] for row in data])) == 1:\n",
    "    # Stopping conditions: reach max depth, min samples, or pure data\n",
    "        # print(\"Finished!\")\n",
    "        aux = majority_vote(data)\n",
    "        # print(\"Label:\", aux)\n",
    "        return TreeNode(label=aux)  # Assign majority class label\n",
    "\n",
    "    best_gain, best_feature_index, best_threshold = 0, None, None\n",
    "    evaluated_data_type = None\n",
    "    best_values = []\n",
    "    # print(\"Part 1\")\n",
    "    for feature_index in range(len(data[0]) - 1):\n",
    "        # print(\"Part 2\")\n",
    "        if type(data[1][feature_index]) != str:\n",
    "            # print(\"NUM\")\n",
    "            # Find the best split among all features\n",
    "            values = set([row[feature_index] for row in data])\n",
    "            for value in values:\n",
    "                # print(\"Part 3\")\n",
    "                gain = self.information_gain(data.copy(), feature_index, value)\n",
    "                if gain > best_gain:\n",
    "                    # print(\"Part 4\")\n",
    "                    best_values = values.copy()\n",
    "                    best_gain, best_feature_index, best_threshold = gain, feature_index, value\n",
    "                    evaluated_data_type = \"num\"\n",
    "                    # print(\"Best gain (num):\", feature_index)\n",
    "        else:\n",
    "            # print(\"CAT\")\n",
    "            # print(\"Part 3\")\n",
    "            values = [item for item in set([row[feature_index] for row in data])]\n",
    "            gain = self.cat_information_gain(data.copy(), feature_index, values)\n",
    "            if gain > best_gain:\n",
    "                # print(\"Part 4\")\n",
    "                best_values = values.copy()\n",
    "                best_gain, best_feature_index, best_threshold = gain, feature_index, 0\n",
    "                evaluated_data_type = \"cat\"\n",
    "                # print(\"Best gain (cat):\", feature_index)\n",
    "                \n",
    "    # print(\"USED:\", best_feature_index)\n",
    "    sets_of_data = {}\n",
    "    children = {}\n",
    "    if evaluated_data_type == \"cat\":\n",
    "        for v in best_values:\n",
    "            sets_of_data[v] = []\n",
    "        for row in data:\n",
    "            for v in best_values:\n",
    "                # print(row[best_feature_index], row[best_feature_index] == v)\n",
    "                if row[best_feature_index] == v:\n",
    "                    sets_of_data[v].append(row)\n",
    "        for v in best_values:\n",
    "            # print(\"Data:\", v)\n",
    "            # print(sets_of_data[v])\n",
    "            children[v] = self._build_tree(sets_of_data[v], depth + 1)\n",
    "        return TreeNode(\n",
    "            feature_index=best_feature_index,\n",
    "            value=best_values,\n",
    "            children=children\n",
    "        )\n",
    "    else:\n",
    "        sets_of_data[\"left\"] = []\n",
    "        sets_of_data[\"right\"] = []\n",
    "        for row in data:\n",
    "            if row[best_feature_index] <= best_threshold:\n",
    "                sets_of_data[\"left\"].append(row)\n",
    "            else:\n",
    "                sets_of_data[\"right\"].append(row)\n",
    "        children[\"left\"] = self._build_tree(sets_of_data[\"left\"], depth + 1)\n",
    "        children[\"right\"] = self._build_tree(sets_of_data[\"right\"], depth + 1)\n",
    "        return TreeNode(\n",
    "            feature_index=best_feature_index,\n",
    "            value=best_threshold,\n",
    "            children=children\n",
    "        )\n",
    "       \n",
    "    # Helper function to find the majority class label\n",
    "def majority_vote(data):\n",
    "  labels = [row[-1] for row in data]\n",
    "  from collections import Counter\n",
    "  counts = Counter(labels)\n",
    "#   print(counts.most_common(1))\n",
    "  return counts.most_common(1)[0][0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dabff8f",
   "metadata": {},
   "source": [
    "### **1. fit(self, data):** To train the decision tree by building the tree structure recursively using the provided dataset.\n",
    "- **self.root = self._build_tree(data, 0)**: Calls the _build_tree method to construct the decision tree starting from the root node.\n",
    "\n",
    "### **2. _build_tree(self, data, depth):** To recursively build the decision tree.\n",
    "- **Stopping Conditions**:\n",
    "  - Checks if any of the stopping conditions are met:\n",
    "    - If the depth of the tree exceeds the maximum depth specified (self.max_depth).\n",
    "    - If the number of samples in the node is less than the minimum required for a split (self.min_samples_split).\n",
    "    - If all samples belong to the same class, indicating pure data.\n",
    "  - If any stopping condition is met, it returns a leaf node with the majority class label.\n",
    "- **Finding the Best Split**:\n",
    "  - Iterates over each feature to find the best split:\n",
    "    - For numerical features:\n",
    "      - Finds the best threshold to split the data based on information gain.\n",
    "    - For categorical features:\n",
    "      - Finds the best feature and its values to split the data based on categorical information gain.\n",
    "  - It tracks the feature index, threshold (for numerical features), and information gain for the best split.\n",
    "- **Splitting the Data**:\n",
    "  - Divides the dataset into subsets based on the best split:\n",
    "    - For categorical features: Creates subsets for each unique value of the best feature.\n",
    "    - For numerical features: Creates subsets based on whether the feature value is less than or equal to the threshold.\n",
    "- **Recursively Building Children Nodes**:\n",
    "  - Recursively calls _build_tree on each subset to build children nodes.\n",
    "- **Creating the TreeNode**:\n",
    "  - Constructs a TreeNode object representing the current node in the decision tree.\n",
    "  - Sets attributes such as feature index, splitting value/threshold, and children nodes.\n",
    "\n",
    "#### **Helper Function**\n",
    "- **majority_vote(data)**:\n",
    "  - Finds the majority class label in a dataset.\n",
    "  - It counts the occurrences of each class label and returns the most common class label.\n",
    "\n",
    "### **Summary**\n",
    "- **fit method**: Trains the decision tree model by building the tree structure.\n",
    "- **_build_tree method**: Recursively constructs the decision tree by finding the best splits and creating nodes.\n",
    "- **majority_vote function**: Finds the majority class label in a dataset, which is used for leaf nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c657df",
   "metadata": {},
   "source": [
    "## **Predict Function** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f036c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def predict(self, datapoint):\n",
    "    # Predict class label for a new data point\n",
    "    # print(\"datapoint = \", datapoint)\n",
    "    node = self.root\n",
    "    counter = 0\n",
    "    while node.label is None:\n",
    "        if type(node.value) is list:#CAT\n",
    "            for value in node.value:\n",
    "                if datapoint[node.feature_index] == value:\n",
    "                    node = node.children[str(value)]\n",
    "                    counter += 1\n",
    "                    break\n",
    "        else:\n",
    "            if datapoint[node.feature_index] <= node.value:\n",
    "                node = node.children[\"left\"]\n",
    "                counter += 1\n",
    "            else:\n",
    "                node = node.children[\"right\"]\n",
    "                counter += 1\n",
    "    return node.label\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9c52f",
   "metadata": {},
   "source": [
    "### **1. predict(self, datapoint):** To predict the class label for a given data point by navigating through the decision tree structure.\n",
    "\n",
    "- **Initialization**:\n",
    "  - **node = self.root**: Starts at the root of the decision tree.\n",
    "  - **counter = 0**: Initializes a counter to track the number of nodes visited (though it isn't used beyond incrementing).\n",
    "\n",
    "- **Tree Traversal**:\n",
    "  - **while node.label is None:**: Continues to traverse the tree until a leaf node (which has a label) is reached.\n",
    "    - **Categorical Features**:\n",
    "      - **if type(node.value) is list:**: Checks if the current node's value is a list, indicating a categorical feature.\n",
    "        - **Iterate over possible values**: Loops through the possible values of the categorical feature.\n",
    "        - **Match value and move to child node**: If the feature value in the data point matches the current value, it moves to the corresponding child node and breaks out of the loop.\n",
    "    - **Numerical Features**:\n",
    "      - **else:**: Handles numerical features where the node's value is not a list.\n",
    "        - **Compare and move left or right**:\n",
    "          - **if datapoint[node.feature_index] <= node.value:**: If the feature value in the data point is less than or equal to the node's value, it moves to the left child.\n",
    "          - **else:**: Otherwise, it moves to the right child.\n",
    "\n",
    "- **Return the Class Label**:\n",
    "  - **return node.label**: Once a leaf node is reached, the method returns the class label associated with that leaf node.\n",
    "\n",
    "### **Summary**\n",
    "- **Initialization**: Starts from the root node of the decision tree.\n",
    "- **Traversal**: Navigates through the tree based on the feature values of the data point. It differentiates between categorical and numerical features to decide which child node to move to.\n",
    "- **Termination**: Stops when a leaf node is reached and returns the class label of that leaf node.\n",
    "\n",
    "### **Example**\n",
    "1. **Starting at the Root Node**: The function begins at the root node.\n",
    "2. **Checking Feature Type**: Determines if the feature is categorical or numerical.\n",
    "3. **Traversing the Tree**: Moves to the appropriate child node based on the feature value in the data point.\n",
    "4. **Repeating Steps**: Continues the process until it reaches a leaf node.\n",
    "5. **Returning the Label**: Once at a leaf node, it returns the class label stored in that node.\n",
    "\n",
    "This method ensures that the class label is predicted accurately by following the structure and decisions defined by the trained decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c021c27b",
   "metadata": {},
   "source": [
    "## **Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46a3b57",
   "metadata": {},
   "source": [
    "We created a class Data to analyse the type of data and do the split in test and train data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33062f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, data, split, name=None):\n",
    "        data = data.fillna(\"Empty\")\n",
    "        # print(data)\n",
    "        # test = self._transform(data)                              # transform categorical features into numerical features\n",
    "        # print(test)                                               # visualize the dataset\n",
    "        self.name = name                                            # dataset name\n",
    "        self.classes = np.unique(data[list(data)[-1]])              # list of classes\n",
    "        self.split = split                                          # split rate between training and test sets\n",
    "        self.train, self.test = self._splitSets(data, self.split)   # split datasets into training and test sets\n",
    "        self.columns = self._getColumns(data)                       # list of column names\n",
    "        # print(len(self.train), len(self.test))                    # visualize training set and test set proportion\n",
    "        self.tree = DecisionTree(max_depth=5)                       # decision tree\n",
    "        self.tree.fit(self.train, self.columns)                     # trains the decision tree\n",
    "        \n",
    "    def _getColumns(self, data):\n",
    "        names = [str(column) for column in data.columns]\n",
    "        types = [type(column) for column in data.iloc[1]]\n",
    "        names.pop()\n",
    "        names.pop(0)\n",
    "        types.pop()\n",
    "        types.pop(0)\n",
    "        return (names, types)\n",
    "    \n",
    "    def testPredict(self):\n",
    "        sum = 0\n",
    "        for row in self.test:\n",
    "            if self.tree.predict(row) == row[-1]:\n",
    "                sum += 1\n",
    "\n",
    "        return sum/len(self.test)\n",
    "        \n",
    "    def _splitSets(self, data, split):                              #split dataset between training and test sets with the same proportion of rows from each class\n",
    "        train = None\n",
    "        test = None\n",
    "        if data.columns.__contains__(\"ID\"):                         # remove ID column if it exists\n",
    "            data = data.drop(\"ID\", axis=1)\n",
    "        dataOfClass = self._splitByClass(data)                      # split dataset by class\n",
    "        for cl in dataOfClass:                                      # split dataset in two keeping the proportion of rows from each class\n",
    "            currentData = dataOfClass[cl]\n",
    "            if len(currentData) == 0:\n",
    "                continue\n",
    "            sampleSize = int(split * len(currentData))\n",
    "            if train is None:\n",
    "                test = currentData.sample(sampleSize, replace=False)\n",
    "                train = currentData.drop(test.index)\n",
    "            else:\n",
    "                tempTest = currentData.sample(sampleSize, replace=False)\n",
    "                tempTrain = currentData.drop(tempTest.index)\n",
    "                train = pd.concat([train, tempTrain], axis=0)\n",
    "                test = pd.concat([test, tempTest], axis=0)\n",
    "        #train.drop(list(data)[-1], axis=1)                          # remove the class column from the training dataframe\n",
    "        return train.values.tolist(), test.values.tolist()          # return training and test sets as lists\n",
    "        \n",
    "    def _splitByClass(self, data):                                  # split the dataset in different datasets containing only rows from the same class\n",
    "        classColumn = list(data)[-1]\n",
    "        classes = np.unique(data[classColumn])\n",
    "        dataOfClass = {}\n",
    "        for class_label in classes:\n",
    "            dataOfClass[class_label] = data[data[classColumn] == class_label] #.drop(classColumn, axis=1)    # remove the class column from the dataframe\n",
    "        return dataOfClass                                          # returns a dictionary {\"class\" : dataframeContainingOnlyRowsOfClass}\n",
    "    \n",
    "    def _transform(self, data):                                     # transform all the columns, turning categorical data into numerical data, except for the class column\n",
    "        enc = LabelEncoder()\n",
    "        new_data = data.copy()\n",
    "        for i, col in enumerate(data.columns):\n",
    "            if i == len(data.columns)-1:\n",
    "                return\n",
    "            if (type(data[col][0]) != str):\n",
    "                continue\n",
    "            new_data[col] = enc.fit_transform(new_data[col])\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c71924",
   "metadata": {},
   "source": [
    "### **1. __init__(self, data, split, name):** The constructor initializes the Data object with a dataset and a split ratio for training and test sets. It also performs some initial preprocessing.\n",
    "- data = data.fillna(\"Empty\"): This replaces any missing values in the dataset with the string \"Empty\".\n",
    "- name: Dataset name\n",
    "- print(data): Prints the dataset to the console for visualization.\n",
    "- self.classes = np.unique(data[list(data)[-1]]): Extracts the unique classes from the last column of the dataset.\n",
    "- self.split = split: Sets the split ratio for training and test sets.\n",
    "- self.train, self.test = self._splitSets(data, self.split): Splits the dataset into training and test sets using the _splitSets method.\n",
    "\n",
    "### **2. _getColumns(self, data):** To get column names and their respective type\n",
    "- Removes the first and last columns \n",
    "- Returns: A tuple containing lists of column names and their corresponding types\n",
    "\n",
    "### **3. _testPredict(self):** To evaluate the performance of the trained decision tree on our test set\n",
    "- It iterates through each row in the test set, predicts the class label using the decision tree, and compares it with the actual class label.\n",
    "- Returns: the accuracy of the decision tree on the test set.\n",
    "\n",
    "### **4. _splitSets(self, data, split):** To split the dataset into training and test sets while maintaining the same proportion of rows from each class.\n",
    "- Removes ID column: If the dataset contains an \"ID\" column, it is removed.\n",
    "- Split by class: Calls _splitByClass to get a dictionary of dataframes, each containing only rows of a single class.\n",
    "- Proportional splitting:\n",
    "    For each class-specific dataframe:\n",
    "        - Samples a portion of rows for the test set based on the split ratio.\n",
    "        - The remaining rows go into the training set.\n",
    "- Concatenates these class-specific splits into the overall training and test sets.\n",
    "- Returns: The training and test sets as lists of rows.\n",
    "\n",
    "### **5. _splitByClass(self, data):** To split the dataset into separate dataframes, each containing only rows of a single class.\n",
    "- Identify the class column: Assumes the last column of the dataset contains the class labels.\n",
    "- Create class-specific dataframes:\n",
    "    - Iterates through each unique class label.\n",
    "    - Filters the original dataset to create a dataframe containing only rows of the current class.\n",
    "- Returns: A dictionary where keys are class labels and values are dataframes with only rows of the corresponding class.\n",
    "\n",
    "### **6. _transform(self, data):** To transform categorical data into numerical data, except for the class column.\n",
    "- Label encoding: Initializes a LabelEncoder from scikit-learn.\n",
    "- Transform each column:\n",
    "    - Iterates over the columns of the dataset.\n",
    "    - Skips the last column (assumed to be the class column).\n",
    "    - Transforms categorical columns (identified as string columns) into numerical data using the label encoder.\n",
    "- Returns: A new dataframe with transformed columns. If a column is not categorical, it remains unchanged.\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "__init__: Initializes the dataset, handles missing values, and splits the data into training and test sets.\n",
    "_splitSets: Splits the dataset into training and test sets while maintaining class proportions.\n",
    "_splitByClass: Creates separate dataframes for each class in the dataset.\n",
    "_transform: Encodes categorical data into numerical values for all columns except the class column.\n",
    "\n",
    "These functions collectively ensure that the data is preprocessed, split, and ready for machine learning tasks while preserving class distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de901d5",
   "metadata": {},
   "source": [
    "We were given four datasets to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f60cd",
   "metadata": {},
   "source": [
    "## **Menu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd864a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTreeOptions():\n",
    "    print()\n",
    "    print(\"Select a Decision Tree:\")\n",
    "    print(\"1.Iris Tree\")\n",
    "    print(\"2.Restaurant Tree\")\n",
    "    print(\"3.Connect4 Tree\")\n",
    "    print(\"4.Weather Tree\")\n",
    "    print()\n",
    "    print(\"0.Get back to menu\")\n",
    "    \n",
    "def testTree(dataset: Data):\n",
    "    print(dataset.name, \"tree precision:\", dataset.testPedict())\n",
    "    \n",
    "def getAndPredictInput(dataset: Data):\n",
    "    toPrint = \"Please insert the following data separated by empty spaces \"+str(dataset.columns[0])+\":\\n\"\n",
    "    x = input(toPrint).split(\" \")\n",
    "    for i in range(len(x)):\n",
    "        x[i] = dataset.columns[1][i](x[i])\n",
    "    return dataset.tree.predict(x)\n",
    "\n",
    "\n",
    "# usig the entire dataset to train the decision tree\n",
    "iris = Data(pd.read_csv(\"datasets/iris.csv\"), 0.0, \"iris\")\n",
    "restaurant = Data(pd.read_csv(\"datasets/restaurant(1).csv\"), 0.0, \"restaurant(1)\")\n",
    "connect4 = Data(pd.read_csv(\"datasets/connect4.csv\"), 0.0, \"connect4\")\n",
    "weather = Data(pd.read_csv(\"datasets/weather.csv\"), 0.0, \"weather\")\n",
    "\n",
    "menuSelect = 0\n",
    "secondaryMenuSelect = 0\n",
    "\n",
    "while True:\n",
    "    if menuSelect == 0:     # Menu\n",
    "        secondaryMenuSelect = 0\n",
    "        print()\n",
    "        print(\"Select an Operation:\")\n",
    "        print(\"1.See Decision Tree\")\n",
    "        print(\"2.Test Decision Tree\")\n",
    "        print(\"3.Predict my own data with Tree\")\n",
    "        print(\"4.Exit\")\n",
    "        menuSelect = int(input())\n",
    "        if (menuSelect > 4 or menuSelect < 0):\n",
    "            menuSelect = 0\n",
    "            continue\n",
    "        continue\n",
    "    elif menuSelect == 1:   # Print a Decision Tree\n",
    "        printTreeOptions()\n",
    "        secondaryMenuSelect = int(input())\n",
    "        \n",
    "        if (secondaryMenuSelect > 4 or secondaryMenuSelect < 0):\n",
    "            secondaryMenuSelect = 0\n",
    "            continue\n",
    "        if secondaryMenuSelect == 0:\n",
    "            menuSelect = 0\n",
    "            continue\n",
    "        \n",
    "        if secondaryMenuSelect == 1:    # Iris\n",
    "            print(iris.tree.print_tree())\n",
    "        elif secondaryMenuSelect == 2:  # Restaurant\n",
    "            print(restaurant.tree.print_tree())\n",
    "        elif secondaryMenuSelect == 3:  # Connect4\n",
    "            print(connect4.tree.print_tree())\n",
    "        else:                           # Weather\n",
    "            print(weather.tree.print_tree())\n",
    "\n",
    "    elif menuSelect == 2:   # Test a Decision Tree using the testing data from the csv file\n",
    "\n",
    "        # train / test split = 0.7 / 0.3\n",
    "        iris = Data(pd.read_csv(\"datasets/iris.csv\"), 0.3, \"iris\")\n",
    "        restaurant = Data(pd.read_csv(\"datasets/restaurant(1).csv\"), 0.3, \"restaurant(1)\")\n",
    "        connect4 = Data(pd.read_csv(\"datasets/connect4.csv\"), 0.3, \"connect4\")\n",
    "        weather = Data(pd.read_csv(\"datasets/weather.csv\"), 0.3, \"weather\")\n",
    "\n",
    "        printTreeOptions()\n",
    "        secondaryMenuSelect = int(input())\n",
    "        \n",
    "        if (secondaryMenuSelect > 4 or secondaryMenuSelect < 0):\n",
    "            secondaryMenuSelect = 0\n",
    "            continue\n",
    "        if secondaryMenuSelect == 0:\n",
    "            menuSelect = 0\n",
    "            continue\n",
    "\n",
    "        if secondaryMenuSelect == 1:    # Iris\n",
    "            testTree(iris)\n",
    "        elif secondaryMenuSelect == 2:  # Restaurant\n",
    "            testTree(restaurant)\n",
    "        elif secondaryMenuSelect == 3:  # Connect4\n",
    "            testTree(connect4)\n",
    "        else:                           # Weather\n",
    "            testTree(weather)\n",
    "\n",
    "    elif menuSelect == 3:   # Insert data and get a prediction from one of the Decision Trees\n",
    "        printTreeOptions()\n",
    "        secondaryMenuSelect = int(input())\n",
    "        \n",
    "        if (secondaryMenuSelect > 4 or secondaryMenuSelect < 0):\n",
    "            secondaryMenuSelect = 0\n",
    "            continue\n",
    "        if secondaryMenuSelect == 0:\n",
    "            menuSelect = 0\n",
    "            continue\n",
    "        \n",
    "        if secondaryMenuSelect == 1:    # Iris\n",
    "            print(getAndPredictInput(iris))\n",
    "        elif secondaryMenuSelect == 2:  # Restaurant\n",
    "            print(getAndPredictInput(restaurant))\n",
    "        elif secondaryMenuSelect == 3:  # Connect4\n",
    "            print(getAndPredictInput(connect4))\n",
    "        else:                           # Weather\n",
    "            print(getAndPredictInput(weather))\n",
    "\n",
    "    elif menuSelect == 4:   # Exit the program\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbab47b",
   "metadata": {},
   "source": [
    "## **Results on Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7ca41a",
   "metadata": {},
   "source": [
    "### **How might different test splits influence the results of the Decision Tree classifier?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c305f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = pd.read_csv('datasets/iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02b4d155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fc/yhkk1bks6bs2m74lnp9spqhw0000gn/T/ipykernel_7440/2321661576.py:1: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  count_classes = pd.value_counts(iris_data['class'], sort = True).sort_index()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_classes = pd.value_counts(iris_data['class'], sort = True).sort_index()\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Iris histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dc01ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_list:  [0.8888888888888888, 0.9333333333333333, 0.9555555555555556, 0.9333333333333333, 0.9111111111111111, 0.9555555555555556, 0.9111111111111111, 1.0, 0.9555555555555556, 0.8444444444444444]\n",
      "Average accuracy:  0.93\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = []\n",
    "for i in range(10):\n",
    "    Iris = Data(iris_data, 0.3)\n",
    "    dTree = DecisionTree(max_depth = 5)\n",
    "    dTree.fit(Iris.train, Iris.columns)\n",
    "    accuracy_list.append(Iris.testPredict())\n",
    "\n",
    "print(\"Accuracy_list: \", accuracy_list)\n",
    "print(f\"Average accuracy:  {sum(accuracy_list)/len(accuracy_list):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990bd1c9",
   "metadata": {},
   "source": [
    "Cross validation is a machine learning technique used in test split of a dataset. Its procedure consists in splitting the data into training and test sets multiple times and obtained average metrics to the model performance.\n",
    "Using the Iris dataset, we can measure the performance of the decision tree classifier through the average accuracy score. By our results, it is possible to state that the test split indeed influences the classifier's performance and that is one of the reasons to use different and more reliable techniques, such as k-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e14a661",
   "metadata": {},
   "source": [
    "### **How can the size chosen for the test set influence the performance of the Decision Tree classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f5c1d7",
   "metadata": {},
   "source": [
    "Here we can see that most of the instances in 'iris.csv' are floats and then there are three labels: 'Iris-virginica', 'Iris-setosa' and 'Iris-versicolor'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af516605",
   "metadata": {},
   "source": [
    "The classes are equally distributed so there are no class imbalance problems, which indicates that is a good dataset. Firstly, we initialize the Tree:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28229036",
   "metadata": {},
   "source": [
    "Then we train it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f3e54b",
   "metadata": {},
   "source": [
    "After that we predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9310406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for row in Iris.test:\n",
    "    #print(\"row_test: \", row)\n",
    "    if dTree.predict(row) == row[-1]: # everytime the prediction is equal to the test in each row of the dataset we increment the counter \n",
    "        sum += 1\n",
    "        \n",
    "print('Number of right predictions: %d/%d' % (sum,len(Iris.test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1d4363",
   "metadata": {},
   "source": [
    "## **Restaurant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_data = pd.read_csv('datasets/restaurant(1).csv')\n",
    "Restaurant = Data(restaurant_data, 0.0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16351ba",
   "metadata": {},
   "source": [
    "**0.7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d04804",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_data = pd.read_csv('datasets/restaurant(1).csv')\n",
    "\n",
    "Restaurant = Data(restaurant_data, 0.3) #we create an instance of class Data and we indicate the split is 0.3 for test data \n",
    "#then we can visualize the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b596c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_classes = pd.value_counts(restaurant_data['Class'], sort = True).sort_index()\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Restaurant histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree = DecisionTree(max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8d44fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree.fit(Restaurant.train)              #training data is an attribute of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for row in Restaurant.test:\n",
    "    #print(\"row_test: \", row)\n",
    "    if dTree.predict(row) == row[-1]: # everytime the prediction is equal to the test in each row of the dataset we increment the counter \n",
    "        sum += 1\n",
    "        \n",
    "print('Number of right predictions: %d/%d' % (sum,len(Restaurant.test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b061d695",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81091045",
   "metadata": {},
   "source": [
    "## **Weather**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51722b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = pd.read_csv('datasets/weather.csv')\n",
    "Weather = Data(weather_data, 0.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f473636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = pd.read_csv('datasets/weather.csv')\n",
    "\n",
    "Weather = Data(weather_data, 0.3) #we create an instance of class Data and we indicate the split is 0.3 for test data \n",
    "#then we can visualize the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa9f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_classes = pd.value_counts(weather_data['Play'], sort = True).sort_index()\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Weather histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e1e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree = DecisionTree(max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5959319",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree.fit(Weather.train)              #training data is an attribute of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aaee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0 \n",
    "for row in Weather.test:\n",
    "    #print(\"row_test: \", row)\n",
    "    if dTree.predict(row) == row[-1]: # everytime the prediction is equal to the test in each row of the dataset we increment the counter \n",
    "        sum += 1\n",
    "        \n",
    "print('Number of right predictions: %d/%d' % (sum,len(Weather.test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbec6ac",
   "metadata": {},
   "source": [
    "## **Connect-4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ebd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "connect4_data = pd.read_csv('datasets/connect4.csv')\n",
    "Connect4 = Data(connect4_data, 0.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed970880",
   "metadata": {},
   "outputs": [],
   "source": [
    "connect4_data = pd.read_csv('datasets/connect4.csv')\n",
    "\n",
    "Connect4 = Data(connect4_data, 0.3) #we create an instance of class Data and we indicate the split is 0.3 for test data \n",
    "#then we can visualize the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ac57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Class = connect4_data.iloc[:, -1]\n",
    "count_classes = Class.value_counts()\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Connect-4 histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c5c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree = DecisionTree(max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca9f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree.fit(Connect4.train)              #training data is an attribute of DatadTree.fit(Weather.train)              #training data is an attribute of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34532cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0 \n",
    "for row in Connect4.test:\n",
    "    #print(\"row_test: \", row)\n",
    "    if dTree.predict(row) == row[-1]: # everytime the prediction is equal to the test in each row of the dataset we increment the counter \n",
    "        sum += 1\n",
    "        \n",
    "print('Number of right predictions: %d/%d' % (sum,len(Connect4.test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
