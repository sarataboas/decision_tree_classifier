{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2637a65b",
   "metadata": {},
   "source": [
    "# **Second assignment: Decision trees** #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c09071",
   "metadata": {},
   "source": [
    "**Assigment done by Marta Longo, Lucas Oliveira, Sara TÃ¡boas for Artificial Inteligence Class**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244648c0",
   "metadata": {},
   "source": [
    "# Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cf986d",
   "metadata": {},
   "source": [
    "\n",
    "<!-- Hello World -->\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Libraries Needed](#Libraries)\n",
    "3. [ID3 Algorithm](#ID3-Algorithm)\n",
    "4. [Entropy and Information Gain](#Entropy-and-Information-Gain)\n",
    "5. [Train Data](#Train-Data)\n",
    "6. [Predict Function](#Predict-Function)\n",
    "7. [Datasets](#Datasets)\n",
    "8. [Results on Datasets](#Results-on-Datasets)\n",
    "    - [1. Iris](#Iris)\n",
    "    - [2. Restaurant](#Restaurant)\n",
    "    - [3. Weather](#Weather)\n",
    "    - [4. Connect-4](#Connect-4)\n",
    "9. [Menu](#Menu)\n",
    "10. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b2a65",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187e378",
   "metadata": {},
   "source": [
    "The purpose of this assigment is to write a program that learns a decision tree from three training datasets that were given to us using the ID3 algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1630366",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d2136e",
   "metadata": {},
   "source": [
    "# ID3 Algorithm\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43e91e0",
   "metadata": {},
   "source": [
    "The ID3 algorithm is a popular method used to create decision trees, which are a type of predictive model used in machine learning and statistics. ID3 works by selecting the attribute that provides the highest information gain to split the data, aiming to reduce entropy (impurity) in the dataset. The process is repeated recursively for each subset, with the attribute that best separates the data chosen at each step, forming a tree structure where each node represents a decision based on an attribute. The recursion stops when all instances in a subset belong to the same class, no more attributes are available, or there are no more instances to split. Here are the key elements:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7a506c",
   "metadata": {},
   "source": [
    "## **Implementation**<a id=\"Implementation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b5495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, feature_index=None, value=None, label=None, counter=0, children=None, feature_name = None):\n",
    "        self.feature_index = feature_index  # Index of the feature used for splitting\n",
    "        self.value = value  # Value used for splitting (if categorical)\n",
    "        self.label = label  # Class label (if leaf node)\n",
    "        self.counter = counter  # Counter for the number of examples\n",
    "        self.children = children  # Dictionary of children nodes\n",
    "        self.feature_name = feature_name  #the feature that was selected\n",
    "      \n",
    "    \n",
    "    def __str__(self):\n",
    "        return (\"Index: \" + str(self.feature_index) + \"\\nValue: \" + str(self.value) + \"\\nLabel:\" + str(self.label) + \"\\nChildren:\" + str(self.children))\n",
    "    \n",
    " \n",
    "    def print_tree(self, indent=\"\", instance_counter=None):\n",
    "        if instance_counter is None:\n",
    "            instance_counter = [0]  # Initialize the counter as a list to pass by reference\n",
    "\n",
    "        instance_counter[0] += 1  # Increment the counter for each node visited\n",
    "        if self.label is not None:\n",
    "            return f\"{self.label} ({instance_counter[0]})\"\n",
    "        \n",
    "        result = []\n",
    "        if isinstance(self.value, list):  # Handling categorical split\n",
    "            result.append(f\"\\n{indent}    <{self.feature_name}>\")\n",
    "            for v in self.children:\n",
    "                child = self.children[v]\n",
    "                result.append(f\"{indent}        {v}: {child.print_tree(indent + '        ', instance_counter)}\")\n",
    "        else:  # Handling numerical split\n",
    "            result.append(f\"\\n{indent}    <{self.feature_name}>\")\n",
    "            result.append(f\"{indent}        <= {self.value}: {self.children['left'].print_tree(indent + '        ', instance_counter)}\")\n",
    "            result.append(f\"{indent}        > {self.value}: {self.children['right'].print_tree(indent + '        ', instance_counter)}\")\n",
    "        \n",
    "        return \"\\n\".join(result)\n",
    "    \n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def entropy(self, labels):\n",
    "        # Calculate entropy (measure of impurity)\n",
    "        from collections import Counter\n",
    "        counts = Counter(labels)\n",
    "        probabilities = [count / len(labels) for count in counts.values()]\n",
    "        entropy = 0\n",
    "        for p in probabilities:\n",
    "            log_value = math.log2(p)\n",
    "            entropy += -p * log_value\n",
    "        return entropy\n",
    "\n",
    "    def cat_information_gain(self, data, feature_index, values):\n",
    "        labels = {}\n",
    "        for v in values:\n",
    "            labels[v] = []\n",
    "        for row in data:\n",
    "            for v in values:\n",
    "                if row[feature_index] == v:\n",
    "                    labels[v].append(row[-1])\n",
    "        result = self.entropy([label for row in data for label in row[-1]])\n",
    "        for v in values:\n",
    "            \n",
    "            result -= self.entropy(labels[v]) * len(labels[v])/len(data)\n",
    "        return result\n",
    "\n",
    "    def information_gain(self, data, feature_index, threshold):\n",
    "        # Calculate information gain for a split\n",
    "        left_labels, right_labels = [], []\n",
    "        for row in data:\n",
    "            if row[feature_index] <= threshold:\n",
    "                left_labels.append(row[-1])\n",
    "            else:\n",
    "                right_labels.append(row[-1])\n",
    "        parent_entropy = self.entropy([label for row in data for label in row[-1]])\n",
    "        left_entropy = self.entropy(left_labels)\n",
    "        right_entropy = self.entropy(right_labels)\n",
    "        weight = len(left_labels) / len(data)\n",
    "        return parent_entropy - (weight * left_entropy + (1 - weight) * right_entropy)\n",
    "\n",
    "    def fit(self, data, feature_name):\n",
    "        # Train the decision tree\n",
    "        self.feature_name = feature_name\n",
    "        self.root = self._build_tree(data, 0)\n",
    "    \n",
    "    def _build_tree(self, data, depth):\n",
    "        # Recursive function to build the tree\n",
    "        if depth >= self.max_depth or len(data) < self.min_samples_split or len(set([row[-1] for row in data])) == 1:\n",
    "        # Stopping conditions: reach max depth, min samples, or pure data\n",
    "            aux = majority_vote(data)\n",
    "            return TreeNode(label=aux)  # Assign majority class label\n",
    "\n",
    "        best_gain, best_feature_index, best_threshold = 0, None, None\n",
    "        evaluated_data_type = None\n",
    "        best_values = []\n",
    "        for feature_index in range(len(data[0]) - 1):\n",
    "            if type(data[1][feature_index]) != str:\n",
    "                # Find the best split among all features\n",
    "                values = set([row[feature_index] for row in data])\n",
    "                for value in values:\n",
    "                    gain = self.information_gain(data.copy(), feature_index, value)\n",
    "                    if gain > best_gain:\n",
    "                        best_values = values.copy()\n",
    "                        best_gain, best_feature_index, best_threshold = gain, feature_index, value\n",
    "                        evaluated_data_type = \"num\"         \n",
    "            else:\n",
    "                values = [item for item in set([row[feature_index] for row in data])]\n",
    "                gain = self.cat_information_gain(data.copy(), feature_index, values)\n",
    "                if gain > best_gain:\n",
    "                    best_values = values.copy()\n",
    "                    best_gain, best_feature_index, best_threshold = gain, feature_index, 0\n",
    "                    evaluated_data_type = \"cat\"\n",
    "        \n",
    "\n",
    "        sets_of_data = {}\n",
    "        children = {}\n",
    "        if evaluated_data_type == \"cat\":\n",
    "            for v in best_values:\n",
    "                sets_of_data[v] = []\n",
    "            sets_of_data[\"default\"] = []\n",
    "            for row in data:\n",
    "                for v in best_values:\n",
    "                    if row[best_feature_index] == v:\n",
    "                        sets_of_data[v].append(row)\n",
    "            for v in best_values:\n",
    "                children[v] = self._build_tree(sets_of_data[v], depth + 1)\n",
    "            children[\"default\"] = TreeNode(label=majority_vote(data))\n",
    "                \n",
    "            return TreeNode(\n",
    "                feature_index=best_feature_index,\n",
    "                value=best_values,\n",
    "                children=children,\n",
    "                feature_name=self.feature_name[0][best_feature_index]\n",
    "                \n",
    "            )\n",
    "        else:\n",
    "            sets_of_data[\"left\"] = []\n",
    "            sets_of_data[\"right\"] = []\n",
    "            for row in data:\n",
    "                if row[best_feature_index] <= best_threshold:\n",
    "                    sets_of_data[\"left\"].append(row)\n",
    "                else:\n",
    "                    sets_of_data[\"right\"].append(row)\n",
    "            children[\"left\"] = self._build_tree(sets_of_data[\"left\"], depth + 1)\n",
    "            children[\"right\"] = self._build_tree(sets_of_data[\"right\"], depth + 1)\n",
    "            return TreeNode(\n",
    "                feature_index=best_feature_index,\n",
    "                value=best_threshold,\n",
    "                children=children,\n",
    "                feature_name=self.feature_name[0][best_feature_index]\n",
    "            )\n",
    "\n",
    "    def predict(self, datapoint):\n",
    "        # Predict class label for a new data point\n",
    "        node = self.root\n",
    "        counter = 0\n",
    "        while node.label is None:\n",
    "            if type(node.value) is list:\n",
    "                if not node.value.__contains__(datapoint[node.feature_index]):\n",
    "                    node = node.children[\"default\"]\n",
    "                    counter += 1\n",
    "                    continue\n",
    "                for value in node.value:\n",
    "                    if datapoint[node.feature_index] == value:\n",
    "                        node = node.children[str(value)]\n",
    "                        counter += 1\n",
    "                        break\n",
    "            else:\n",
    "                if datapoint[node.feature_index] <= node.value:\n",
    "                    node = node.children[\"left\"]\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    node = node.children[\"right\"]\n",
    "                    counter += 1\n",
    "        return node.label\n",
    "    \n",
    "\n",
    "    def print_tree(self):\n",
    "        if self.root:\n",
    "            return self.root.print_tree()\n",
    "        \n",
    "        return \"Tree is empty.\"\n",
    "\n",
    "\n",
    "\n",
    "# Helper function to find the majority class label\n",
    "def majority_vote(data):\n",
    "  labels = [row[-1] for row in data]\n",
    "  from collections import Counter\n",
    "  counts = Counter(labels)\n",
    "  return counts.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30309d14",
   "metadata": {},
   "source": [
    "**TreeNode Class Attributes**\n",
    "- **feature_index**: This is the index of the feature used to split the data at this node. It identifies which feature (among the input features) is being used to decide the split at this node. If this node is a leaf node (i.e., it does not split further), feature_index will be None.\n",
    "\n",
    "\n",
    "- **value**: This is the value used for splitting the data at this node if the feature is continuous. For categorical features, this could be the category value. It represents the threshold or category against which the feature values are compared. If this node is a leaf node, value will be None.\n",
    "\n",
    "- **label**: This is the class label that the node predicts if it is a leaf node. For non-leaf nodes, this will be None.\n",
    "\n",
    "- **counter**: This represents the count of examples that reach this node during the tree building process.\n",
    "\n",
    "- **children**: This is a dictionary that stores the children nodes of the current node. Each key in the dictionary represents a particular value or range of values of the feature associated with the current node. The values associated with these keys are the child nodes resulting from splitting the data based on the corresponding feature value. For categorical features, the keys represent distinct categories, while for continuous features, the keys can represent different partitions of the feature space.\n",
    "\n",
    "- **feature_name**: This is the name/label of the feature associated with the current node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70518561",
   "metadata": {},
   "source": [
    "**Decision Tree Class Attributes**\n",
    "- **max_depth**: This specifies the maximum depth that the tree is allowed to grow to. If None, the tree will grow until all leaves are pure or until there are fewer than min_samples_split samples in the leaves. Limiting the depth helps in controlling overfitting.\n",
    "\n",
    "- **min_samples_split**: This defines the minimum number of samples required to split an internal node. This parameter prevents the tree from splitting too much and helps in controlling overfitting. If the number of samples at a node is less than min_samples_split, the node will not be split further.\n",
    "\n",
    "- **root**: This is the root node of the decision tree. It is the starting point for making predictions. Initially, this is None until the tree is fitted with data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e989e",
   "metadata": {},
   "source": [
    "## **Entropy and Information Gain**\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df74885",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def entropy(self, labels):\n",
    "        # Calculate entropy (measure of impurity)\n",
    "        from collections import Counter\n",
    "        counts = Counter(labels)\n",
    "        probabilities = [count / len(labels) for count in counts.values()]\n",
    "        print(\"prob = \", probabilities) \n",
    "        # from math import log2\n",
    "        entropy = 0\n",
    "        for p in probabilities:\n",
    "            log_value = math.log2(p)\n",
    "            entropy += -p * log_value\n",
    "        return entropy\n",
    "\n",
    "\n",
    "def cat_information_gain(self, data, feature_index, values):\n",
    "    labels = {}\n",
    "    for v in values:\n",
    "        labels[v] = []\n",
    "    for row in data:\n",
    "        for v in values:\n",
    "            if row[feature_index] == v:\n",
    "                labels[v].append(row[-1])\n",
    "    result = self.entropy([label for row in data for label in row[-1]])\n",
    "    for v in values:\n",
    "        \n",
    "        result -= self.entropy(labels[v]) * len(labels[v])/len(data)\n",
    "    return result\n",
    "\n",
    "    \n",
    "def information_gain(self, data, feature_index, threshold):\n",
    "    # Calculate information gain for a split\n",
    "    left_labels, right_labels = [], []\n",
    "    for row in data:           \n",
    "        if row[feature_index] <= threshold:\n",
    "            left_labels.append(row[-1])\n",
    "        else:\n",
    "            right_labels.append(row[-1])\n",
    "    parent_entropy = self.entropy([label for row in data for label in row[-1]])\n",
    "    left_entropy = self.entropy(left_labels)\n",
    "    right_entropy = self.entropy(right_labels)\n",
    "    weight = len(left_labels) / len(data)\n",
    "    return parent_entropy - (weight * left_entropy + (1 - weight) * right_entropy)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c61b221",
   "metadata": {},
   "source": [
    "### 1. **entropy(self, labels):** To calculate the entropy of a set of labels. Entropy is a measure of the impurity or randomness in the dataset.\n",
    "\n",
    "#### Functionality\n",
    "- **Count occurrences**: Uses Counter from the collections module to count the occurrences of each unique label.\n",
    "- **Calculate probabilities**: Computes the probability of each label by dividing the count of each label by the total number of labels.\n",
    "- **Calculate entropy**: Uses the formula $$\\text{Entropy} = -\\sum(p \\log_2 p)$$  where \\( p \\) is the probability of a label. This is summed over all unique labels.\n",
    "- **Return entropy**: Returns the calculated entropy value.\n",
    "\n",
    "### **2. cat_information_gain(self, data, feature_index, values):** To calculate the information gain for a categorical feature. Information gain measures the reduction in entropy achieved by splitting the data based on a feature.\n",
    "- **Initialize labels dictionary**: Creates a dictionary labels to hold lists of class labels for each unique value of the categorical feature.\n",
    "- **Distribute labels**: Iterates over the rows in the dataset and assigns the class label (last element in the row) to the appropriate list in the labels dictionary based on the feature value.\n",
    "- **Calculate parent entropy**: Computes the entropy of the entire dataset's class labels using the entropy method.\n",
    "- **Calculate information gain**:\n",
    "  - For each unique value in the feature, calculate the entropy of the corresponding subset of labels.\n",
    "  - Adjust the parent entropy by subtracting the weighted entropy of each subset (proportional to its size).\n",
    "- **Return information gain**: Returns the resulting information gain.\n",
    "\n",
    "### **3. information_gain(self, data, feature_index, threshold):** To calculate the information gain for a numerical feature based on a given threshold. This is used to evaluate the best threshold for splitting the data.\n",
    "\n",
    "- **Initialize label lists**: Creates two lists, left_labels and right_labels, to hold class labels for the left and right subsets created by the threshold split.\n",
    "- **Distribute labels**: Iterates over the rows in the dataset and assigns the class label (last element in the row) to either left_labels or right_labels based on whether the feature value is less than or equal to the threshold.\n",
    "- **Calculate parent entropy**: Computes the entropy of the entire dataset's class labels using the entropy` method.\n",
    "- **Calculate subset entropies**: Computes the entropy of left_labels and right_labels.\n",
    "- **Calculate weighted average entropy**: Computes the weighted average of the left and right entropies based on their sizes relative to the total dataset.\n",
    "- **Return information gain**: Subtracts the weighted average entropy from the parent entropy to get the information gain.\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "- **entropy**: Calculates the entropy of a list of labels, providing a measure of impurity.\n",
    "- **cat_information_gain**: Calculates the information gain for a categorical feature, assessing how much splitting the data on this feature reduces impurity.\n",
    "- **information_gain**: Calculates the information gain for a numerical feature using a specific threshold, evaluating the effectiveness of this split in reducing impurity.\n",
    "\n",
    "These functions are essential for decision tree algorithms, helping to determine the best features and thresholds for splitting the data to achieve the most significant reduction in impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5a7b5f",
   "metadata": {},
   "source": [
    "## **Train Data**\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823cebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def fit(self, data):\n",
    "        # Train the decision tree\n",
    "        self.root = self._build_tree(data, 0)\n",
    "    \n",
    "def _build_tree(self, data, depth):\n",
    "    # Recursive function to build the tree\n",
    "    if depth >= self.max_depth or len(data) < self.min_samples_split or len(set([row[-1] for row in data])) == 1:\n",
    "    # Stopping conditions: reach max depth, min samples, or pure data\n",
    "        aux = majority_vote(data)\n",
    "        return TreeNode(label=aux)  # Assign majority class label\n",
    "\n",
    "    best_gain, best_feature_index, best_threshold = 0, None, None\n",
    "    evaluated_data_type = None\n",
    "    best_values = []\n",
    "    for feature_index in range(len(data[0]) - 1):\n",
    "        if type(data[1][feature_index]) != str:\n",
    "            # Find the best split among all features\n",
    "            values = set([row[feature_index] for row in data])\n",
    "            for value in values:\n",
    "                gain = self.information_gain(data.copy(), feature_index, value)\n",
    "                if gain > best_gain:\n",
    "                    best_values = values.copy()\n",
    "                    best_gain, best_feature_index, best_threshold = gain, feature_index, value\n",
    "                    evaluated_data_type = \"num\"\n",
    "                   \n",
    "        else:\n",
    "            \n",
    "            values = [item for item in set([row[feature_index] for row in data])]\n",
    "            gain = self.cat_information_gain(data.copy(), feature_index, values)\n",
    "            if gain > best_gain:\n",
    "                best_values = values.copy()\n",
    "                best_gain, best_feature_index, best_threshold = gain, feature_index, 0\n",
    "                evaluated_data_type = \"cat\"\n",
    "               \n",
    "  \n",
    "    sets_of_data = {}\n",
    "    children = {}\n",
    "    if evaluated_data_type == \"cat\":\n",
    "        for v in best_values:\n",
    "            sets_of_data[v] = []\n",
    "        for row in data:\n",
    "            for v in best_values:\n",
    "                if row[best_feature_index] == v:\n",
    "                    sets_of_data[v].append(row)\n",
    "        for v in best_values:\n",
    "            children[v] = self._build_tree(sets_of_data[v], depth + 1)\n",
    "        return TreeNode(\n",
    "            feature_index=best_feature_index,\n",
    "            value=best_values,\n",
    "            children=children\n",
    "        )\n",
    "    else:\n",
    "        sets_of_data[\"left\"] = []\n",
    "        sets_of_data[\"right\"] = []\n",
    "        for row in data:\n",
    "            if row[best_feature_index] <= best_threshold:\n",
    "                sets_of_data[\"left\"].append(row)\n",
    "            else:\n",
    "                sets_of_data[\"right\"].append(row)\n",
    "        children[\"left\"] = self._build_tree(sets_of_data[\"left\"], depth + 1)\n",
    "        children[\"right\"] = self._build_tree(sets_of_data[\"right\"], depth + 1)\n",
    "        return TreeNode(\n",
    "            feature_index=best_feature_index,\n",
    "            value=best_threshold,\n",
    "            children=children\n",
    "        )\n",
    "       \n",
    "    # Helper function to find the majority class label\n",
    "def majority_vote(data):\n",
    "  labels = [row[-1] for row in data]\n",
    "  from collections import Counter\n",
    "  counts = Counter(labels)\n",
    "  return counts.most_common(1)[0][0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dabff8f",
   "metadata": {},
   "source": [
    "### **1. fit(self, data, feature_name):** To train the decision tree by building the tree structure recursively using the provided dataset.\n",
    "- **self.feature_name = feature_name**: We indicate what features are there to later use them on the print function\n",
    "- **self.root = self._build_tree(data, 0)**: Calls the _build_tree method to construct the decision tree starting from the root node.\n",
    "\n",
    "### **2. _build_tree(self, data, depth):** To recursively build the decision tree.\n",
    "- **Stopping Conditions**:\n",
    "  - Checks if any of the stopping conditions are met:\n",
    "    - If the depth of the tree exceeds the maximum depth specified (self.max_depth).\n",
    "    - If the number of samples in the node is less than the minimum required for a split (self.min_samples_split).\n",
    "    - If all samples belong to the same class, indicating pure data.\n",
    "  - If any stopping condition is met, it returns a leaf node with the majority class label.\n",
    "- **Finding the Best Split**:\n",
    "  - Iterates over each feature to find the best split:\n",
    "    - For numerical features:\n",
    "      - Finds the best threshold to split the data based on information gain.\n",
    "    - For categorical features:\n",
    "      - Finds the best feature and its values to split the data based on categorical information gain.\n",
    "  - It tracks the feature index, threshold (for numerical features), and information gain for the best split.\n",
    "- **Splitting the Data**:\n",
    "  - Divides the dataset into subsets based on the best split:\n",
    "    - For categorical features: Creates subsets for each unique value of the best feature.\n",
    "    - For numerical features: Creates subsets based on whether the feature value is less than or equal to the threshold.\n",
    "- **Recursively Building Children Nodes**:\n",
    "  - Recursively calls _build_tree on each subset to build children nodes.\n",
    "- **Creating the TreeNode**:\n",
    "  - Constructs a TreeNode object representing the current node in the decision tree.\n",
    "  - Sets attributes such as feature index, splitting value/threshold, children nodes and feature name.\n",
    "\n",
    "#### **Helper Function**\n",
    "- **majority_vote(data)**:\n",
    "  - Finds the majority class label in a dataset.\n",
    "  - It counts the occurrences of each class label and returns the most common class label.\n",
    "\n",
    "### **Summary**\n",
    "- **fit method**: Trains the decision tree model by building the tree structure.\n",
    "- **_build_tree method**: Recursively constructs the decision tree by finding the best splits and creating nodes.\n",
    "- **majority_vote function**: Finds the majority class label in a dataset, which is used for leaf nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c657df",
   "metadata": {},
   "source": [
    "## **Predict Function** \n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f036c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def predict(self, datapoint):\n",
    "    # Predict class label for a new data point\n",
    "    node = self.root\n",
    "    counter = 0\n",
    "    while node.label is None:\n",
    "        if type(node.value) is list:#CAT\n",
    "            for value in node.value:\n",
    "                if datapoint[node.feature_index] == value:\n",
    "                    node = node.children[str(value)]\n",
    "                    counter += 1\n",
    "                    break\n",
    "        else:\n",
    "            if datapoint[node.feature_index] <= node.value:\n",
    "                node = node.children[\"left\"]\n",
    "                counter += 1\n",
    "            else:\n",
    "                node = node.children[\"right\"]\n",
    "                counter += 1\n",
    "    return node.label\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9c52f",
   "metadata": {},
   "source": [
    "### **1. predict(self, datapoint):** To predict the class label for a given data point by navigating through the decision tree structure.\n",
    "\n",
    "- **Initialization**:\n",
    "  - **node = self.root**: Starts at the root of the decision tree.\n",
    "  - **counter = 0**: Initializes a counter to track the number of nodes visited (though it isn't used beyond incrementing).\n",
    "\n",
    "- **Tree Traversal**:\n",
    "  - **while node.label is None:**: Continues to traverse the tree until a leaf node (which has a label) is reached.\n",
    "    - **Categorical Features**:\n",
    "      - **if type(node.value) is list:**: Checks if the current node's value is a list, indicating a categorical feature.\n",
    "        - **Iterate over possible values**: Loops through the possible values of the categorical feature.\n",
    "        - **Match value and move to child node**: If the feature value in the data point matches the current value, it moves to the corresponding child node and breaks out of the loop.\n",
    "    - **Numerical Features**:\n",
    "      - **else:**: Handles numerical features where the node's value is not a list.\n",
    "        - **Compare and move left or right**:\n",
    "          - **if datapoint[node.feature_index] <= node.value:**: If the feature value in the data point is less than or equal to the node's value, it moves to the left child.\n",
    "          - **else:**: Otherwise, it moves to the right child.\n",
    "\n",
    "- **Return the Class Label**:\n",
    "  - **return node.label**: Once a leaf node is reached, the method returns the class label associated with that leaf node.\n",
    "\n",
    "### **Summary**\n",
    "- **Initialization**: Starts from the root node of the decision tree.\n",
    "- **Traversal**: Navigates through the tree based on the feature values of the data point. It differentiates between categorical and numerical features to decide which child node to move to.\n",
    "- **Termination**: Stops when a leaf node is reached and returns the class label of that leaf node.\n",
    "\n",
    "### **Example**\n",
    "1. **Starting at the Root Node**: The function begins at the root node.\n",
    "2. **Checking Feature Type**: Determines if the feature is categorical or numerical.\n",
    "3. **Traversing the Tree**: Moves to the appropriate child node based on the feature value in the data point.\n",
    "4. **Repeating Steps**: Continues the process until it reaches a leaf node.\n",
    "5. **Returning the Label**: Once at a leaf node, it returns the class label stored in that node.\n",
    "\n",
    "This method ensures that the class label is predicted accurately by following the structure and decisions defined by the trained decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c021c27b",
   "metadata": {},
   "source": [
    "## **Datasets**\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46a3b57",
   "metadata": {},
   "source": [
    "We created a class Data to analyse the type of data and do the split in test and train data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33062f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, data, split, name=None):\n",
    "        \n",
    "        print(data)                                                 # visualize the dataset\n",
    "        data = data.fillna(\"Empty\")                                 # replace the NAN values with a string so it was a type                                         \n",
    "        self.name = name                                            # dataset name\n",
    "        self.classes = np.unique(data[list(data)[-1]])              # list of classes\n",
    "        self.split = split                                          # split rate between training and test sets\n",
    "        self.train, self.test = self._splitSets(data, self.split)   # split datasets into training and test sets\n",
    "        self.columns = self._getColumns(data)                       # list of column names\n",
    "        self.tree = DecisionTree(max_depth=5)                       # decision tree\n",
    "        self.tree.fit(self.train, self.columns)                     # trains the decision tree\n",
    "        \n",
    "    def _getColumns(self, data):\n",
    "        names = [str(column) for column in data.columns]\n",
    "        types = [type(column) for column in data.iloc[1]]\n",
    "        names.pop()\n",
    "        names.pop(0)\n",
    "        types.pop()\n",
    "        types.pop(0)\n",
    "        return (names, types)\n",
    "    \n",
    "    def testPredict(self):\n",
    "        sum = 0\n",
    "        for row in self.test:\n",
    "            if self.tree.predict(row) == row[-1]:\n",
    "                sum += 1\n",
    "\n",
    "        return sum/len(self.test)\n",
    "        \n",
    "    def _splitSets(self, data, split):                              #split dataset between training and test sets with the same proportion of rows from each class\n",
    "        train = None\n",
    "        test = None\n",
    "        if data.columns.__contains__(\"ID\"):                         # remove ID column if it exists\n",
    "            data = data.drop(\"ID\", axis=1)\n",
    "        dataOfClass = self._splitByClass(data)                      # split dataset by class\n",
    "        for cl in dataOfClass:                                      # split dataset in two keeping the proportion of rows from each class\n",
    "            currentData = dataOfClass[cl]\n",
    "            if len(currentData) == 0:\n",
    "                continue\n",
    "            sampleSize = int(split * len(currentData))\n",
    "            if train is None:\n",
    "                test = currentData.sample(sampleSize, replace=False)\n",
    "                train = currentData.drop(test.index)\n",
    "            else:\n",
    "                tempTest = currentData.sample(sampleSize, replace=False)\n",
    "                tempTrain = currentData.drop(tempTest.index)\n",
    "                train = pd.concat([train, tempTrain], axis=0)\n",
    "                test = pd.concat([test, tempTest], axis=0)\n",
    "        return train.values.tolist(), test.values.tolist()          # return training and test sets as lists\n",
    "        \n",
    "    def _splitByClass(self, data):                                  # split the dataset in different datasets containing only rows from the same class\n",
    "        classColumn = list(data)[-1]\n",
    "        classes = np.unique(data[classColumn])\n",
    "        dataOfClass = {}\n",
    "        for class_label in classes:\n",
    "            dataOfClass[class_label] = data[data[classColumn] == class_label]   # remove the class column from the dataframe\n",
    "        return dataOfClass                                          # returns a dictionary {\"class\" : dataframeContainingOnlyRowsOfClass}\n",
    "    \n",
    "    def _transform(self, data):                                     # transform all the columns, turning categorical data into numerical data, except for the class column\n",
    "        enc = LabelEncoder()\n",
    "        new_data = data.copy()\n",
    "        for i, col in enumerate(data.columns):\n",
    "            if i == len(data.columns)-1:\n",
    "                return\n",
    "            if (type(data[col][0]) != str):\n",
    "                continue\n",
    "            new_data[col] = enc.fit_transform(new_data[col])\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c71924",
   "metadata": {},
   "source": [
    "### **1. __init__(self, data, split, name):** The constructor initializes the Data object with a dataset and a split ratio for training and test sets. It also performs some initial preprocessing.\n",
    "- data = data.fillna(\"Empty\"): This replaces any missing values in the dataset with the string \"Empty\".\n",
    "- name: Dataset name\n",
    "- print(data): Prints the dataset to the console for visualization.\n",
    "- self.classes = np.unique(data[list(data)[-1]]): Extracts the unique classes from the last column of the dataset.\n",
    "- self.split = split: Sets the split ratio for training and test sets.\n",
    "- self.train, self.test = self._splitSets(data, self.split): Splits the dataset into training and test sets using the _splitSets method.\n",
    "\n",
    "### **2. _getColumns(self, data):** To get column names and their respective type\n",
    "- Removes the first and last columns \n",
    "- Returns: A tuple containing lists of column names and their corresponding types\n",
    "\n",
    "### **3. _testPredict(self):** To evaluate the performance of the trained decision tree on our test set\n",
    "- It iterates through each row in the test set, predicts the class label using the decision tree, and compares it with the actual class label.\n",
    "- Returns: the accuracy of the decision tree on the test set.\n",
    "\n",
    "### **4. _splitSets(self, data, split):** To split the dataset into training and test sets while maintaining the same proportion of rows from each class.\n",
    "- Removes ID column: If the dataset contains an \"ID\" column, it is removed.\n",
    "- Split by class: Calls _splitByClass to get a dictionary of dataframes, each containing only rows of a single class.\n",
    "- Proportional splitting:\n",
    "    For each class-specific dataframe:\n",
    "        - Samples a portion of rows for the test set based on the split ratio.\n",
    "        - The remaining rows go into the training set.\n",
    "- Concatenates these class-specific splits into the overall training and test sets.\n",
    "- Returns: The training and test sets as lists of rows.\n",
    "\n",
    "### **5. _splitByClass(self, data):** To split the dataset into separate dataframes, each containing only rows of a single class.\n",
    "- Identify the class column: Assumes the last column of the dataset contains the class labels.\n",
    "- Create class-specific dataframes:\n",
    "    - Iterates through each unique class label.\n",
    "    - Filters the original dataset to create a dataframe containing only rows of the current class.\n",
    "- Returns: A dictionary where keys are class labels and values are dataframes with only rows of the corresponding class.\n",
    "\n",
    "### **6. _transform(self, data):** To transform categorical data into numerical data, except for the class column.\n",
    "- Label encoding: Initializes a LabelEncoder from scikit-learn.\n",
    "- Transform each column:\n",
    "    - Iterates over the columns of the dataset.\n",
    "    - Skips the last column (assumed to be the class column).\n",
    "    - Transforms categorical columns (identified as string columns) into numerical data using the label encoder.\n",
    "- Returns: A new dataframe with transformed columns. If a column is not categorical, it remains unchanged.\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "__init__: Initializes the dataset, handles missing values, and splits the data into training and test sets.\n",
    "**_splitSets:** Splits the dataset into training and test sets while maintaining class proportions.\n",
    "**_splitByClass:** Creates separate dataframes for each class in the dataset.\n",
    "**_transform:** Encodes categorical data into numerical values for all columns except the class column.\n",
    "\n",
    "These functions collectively ensure that the data is preprocessed, split, and ready for machine learning tasks while preserving class distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de901d5",
   "metadata": {},
   "source": [
    "We were given four datasets to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbab47b",
   "metadata": {},
   "source": [
    "## **Results on Datasets**\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c120e23a",
   "metadata": {},
   "source": [
    "For each dataset, we started by using the entire dataset to train and build our decision tree classifier. Then, when added a new line as input, out classifier is able to predict the most likely class value. \n",
    "\n",
    "Additionally, it is possible to use the four datasets available to do a train/test split by choosing the percentage of data of each set. We defined a standard train/test split of 70%/30%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf5f553",
   "metadata": {},
   "source": [
    "## **Iris Analysis**\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a9943",
   "metadata": {},
   "source": [
    "Iris Dataset contains numerical information about plants of three classes: iris setosa, iris virginica\n",
    "and iris versicolor. The attributes are petal length and width and sepal length and width. The\n",
    "task is to learn a decision tree that can tell to which class a plant belongs to, given its sepal and\n",
    "petal lengths and widths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/iris.csv')\n",
    "Iris = Data(data, 0.3, \"Iris\") #creates an instance of class Data and we indicate the split is 0.3 for test data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f5c1d7",
   "metadata": {},
   "source": [
    "Here we can see that most of the instances in 'iris.csv' are floats and then there are three labels: 'Iris-virginica', 'Iris-setosa' and 'Iris-versicolor'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_classes = pd.value_counts(data['class'], sort = True).sort_index()\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Iris histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af516605",
   "metadata": {},
   "source": [
    "The classes are equally distributed so there are no class imbalance problems, which indicates that is a good dataset. Firstly, we initialize the Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4047d91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree = DecisionTree(max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28229036",
   "metadata": {},
   "source": [
    "Then we train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05cc3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree.fit(Iris.train, Iris.columns)              #training data is an attribute of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f3e54b",
   "metadata": {},
   "source": [
    "After that, we predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9310406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "sum = 0\n",
    "for row in Iris.test:\n",
    "    if dTree.predict(row) == row[-1]: # everytime the prediction is equal to the test in each row of the dataset we increment the counter \n",
    "        sum += 1\n",
    "        \n",
    "print('Number of right predictions: %d/%d' % (sum,len(Iris.test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9ae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize lists to store true and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Collect predictions and true labels\n",
    "for row in Iris.test:\n",
    "    true_labels.append(row[-1])\n",
    "    predicted_labels.append(dTree.predict(row))\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(true_labels, predicted_labels, target_names=[str(cls) for cls in np.unique(true_labels)])\n",
    "\n",
    "# Print the classification report\n",
    "print('Classification Report:')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7a2cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dTree.print_tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1d4363",
   "metadata": {},
   "source": [
    "## **Restaurant**\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f2ba0",
   "metadata": {},
   "source": [
    "The Restaurant dataset contains information about customers and\n",
    "restaurants (type of food, waiting time, price etc), and the class attribute (last column) says if\n",
    "the customer will wait or not to eat in that restaurant. The task is to generate a decision tree (as\n",
    "explained in theoretical class and following the ID3 algorithm available in the textbook). This\n",
    "decision tree must be used later to classify (answer if the customer will wait or not) new cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_data = pd.read_csv('datasets/restaurant(1).csv')\n",
    "Restaurant = Data(restaurant_data, 0.3, \"Restaurant\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16351ba",
   "metadata": {},
   "source": [
    "**0.7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b596c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_classes = pd.value_counts(restaurant_data['Class'], sort = True).sort_index()\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Restaurant histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree = DecisionTree(max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8d44fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree.fit(Restaurant.train, Restaurant.columns)              #training data is an attribute of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for row in Restaurant.test:\n",
    "    print(\"row_test: \", row)\n",
    "    if dTree.predict(row) == row[-1]: # everytime the prediction is equal to the test in each row of the dataset we increment the counter \n",
    "        sum += 1\n",
    "        \n",
    "print('Number of right predictions: %d/%d' % (sum,len(Restaurant.test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee4b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dTree.print_tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81091045",
   "metadata": {},
   "source": [
    "## **Weather**\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34f8e16",
   "metadata": {},
   "source": [
    "The Weather dataset contains information about climate conditions to play tennis. The task is to learn a\n",
    "decision tree that can decide what are the best conditions to play tennis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f473636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = pd.read_csv('datasets/weather.csv')\n",
    "Weather = Data(weather_data, 0.3, \"Weather\") #we create an instance of class Data and we indicate the split is 0.3 for test data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81560825",
   "metadata": {},
   "source": [
    "As we can see most of the attributes are strings (categorical) therefore they use our categorical information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa9f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_classes = pd.value_counts(weather_data['Play'], sort = True).sort_index()\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Weather histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e1e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree = DecisionTree(max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5959319",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree.fit(Weather.train, Weather.columns)              #training data is an attribute of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aaee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0 \n",
    "for row in Weather.test:\n",
    "    if dTree.predict(row) == row[-1]: # everytime the prediction is equal to the test in each row of the dataset we increment the counter \n",
    "        sum += 1\n",
    "        \n",
    "print('Number of right predictions: %d/%d' % (sum,len(Weather.test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f670fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dTree.print_tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbec6ac",
   "metadata": {},
   "source": [
    "## **Connect-4**\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c9b0cb",
   "metadata": {},
   "source": [
    "This datasets contains all the combinations of the board of Connect-4 game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ebd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "connect4_data = pd.read_csv('datasets/connect4.csv')\n",
    "connect4_data_headers = [i for i in range(len(connect4_data.columns)-1)]\n",
    "connect4_data_headers.append(\"Class\")\n",
    "connect4_data_old_header = [i for i in connect4_data.columns]\n",
    "connect4_data.columns = connect4_data_headers\n",
    "connect4_data.loc[len(connect4_data)] = connect4_data_old_header\n",
    "connect4_data.insert(0, \"ID\", [i for i in range(len(connect4_data))])\n",
    "Connect4 = Data(connect4_data, 0.3, \"Connect4\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead8a37",
   "metadata": {},
   "source": [
    "All the instances here are strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ac57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Class = connect4_data.iloc[:, -1]\n",
    "count_classes = Class.value_counts()\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Connect-4 histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c5c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree = DecisionTree(max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca9f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree.fit(Connect4.train, Connect4.columns)              #training data is an attribute of DatadTree.fit(Weather.train)              #training data is an attribute of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e64a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0 \n",
    "for row in Connect4.test:\n",
    "    if dTree.predict(row) == row[-1]: # everytime the prediction is equal to the test in each row of the dataset we increment the counter \n",
    "        sum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3602a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dTree.print_tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958bfb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from connect_four import Board \n",
    "from tree import Node \n",
    "import random\n",
    "\n",
    "game_board = Board()\n",
    "\n",
    "def convert_to_response(board):\n",
    "    resp=\"\"\n",
    "    row=0\n",
    "    column=0\n",
    "    for i in range (42):\n",
    "        if row == 6:\n",
    "            row = 0\n",
    "            column += 1\n",
    "        if board[5-row][column] == '.':\n",
    "            board[5-row][column] = 'b'\n",
    "        elif board[5-row][column] == 'X':\n",
    "            board[5-row][column] = 'x'\n",
    "        elif board[5-row][column] == 'O':\n",
    "            board[5-row][column] = 'o'\n",
    "\n",
    "        resp = resp + board[5-row][column] + \" \"\n",
    "        row+=1\n",
    "    \n",
    "    resp = resp.strip()\n",
    "    return resp\n",
    "\n",
    "def getAndPredictConnectFour(dataset: Data, resp):\n",
    "    x = resp.split(\" \")\n",
    "    for i in range(len(x)):\n",
    "        x[i] = dataset.columns[1][i](x[i])\n",
    "    return dataset.tree.predict(x)\n",
    "\n",
    "while not game_board.end:                                            #decision tree plays\n",
    "    root = Node(game_board, {}, None)\n",
    "    root.genChildren(game_board.turn)\n",
    "    pred = {}\n",
    "    for move in root.children:\n",
    "        resp = convert_to_response(root.children[move].value.board) # convert the game state to csv-like format\n",
    "        if getAndPredictConnectFour(Connect4, resp) == 'win':\n",
    "            pred[move] = getAndPredictConnectFour(Connect4, resp)\n",
    "    print(pred)\n",
    "    if not pred:\n",
    "        column = random.choice(game_board.possibleMoves())\n",
    "    else:\n",
    "        column = random.choice(list(pred.keys()))\n",
    "    game_board.move(column, game_board.turn)\n",
    "    print(game_board)\n",
    "    if game_board.winner != None:\n",
    "        print('The winner is the MACHINE:', game_board.winner)\n",
    "        break\n",
    "    if not game_board.end:                                              #human plays\n",
    "        user = int(input(\"Insert column:\"))\n",
    "        game_board.move(user, game_board.turn)\n",
    "        print(game_board)\n",
    "        if game_board.winner != None:\n",
    "            print('The winner is HUMAN:', game_board.winner)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f988a54",
   "metadata": {},
   "source": [
    "## **Menu**\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd7f8c",
   "metadata": {},
   "source": [
    "We created a menu were you can choose which tree you want to see, test the decision tree selected and predict unlabeled data with that same three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52406d55-a5a0-4339-9ba8-25ab3f21d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTreeOptions():\n",
    "    print()\n",
    "    print(\"Select a Decision Tree:\")\n",
    "    print(\"1.Iris Tree\")\n",
    "    print(\"2.Restaurant Tree\")\n",
    "    print(\"3.Connect4 Tree\")\n",
    "    print(\"4.Weather Tree\")\n",
    "    print()\n",
    "    print(\"0.Get back to menu\")\n",
    "    \n",
    "def testTree(dataset: Data):\n",
    "    print(dataset.name, \"tree precision:\", dataset.testPedict())\n",
    "    \n",
    "def getAndPredictInput(dataset: Data):\n",
    "    toPrint = \"Please insert the following data separated by empty spaces \"+str(dataset.columns[0])+\":\\n\"\n",
    "    x = input(toPrint).split(\" \")\n",
    "    for i in range(len(x)):\n",
    "        x[i] = dataset.columns[1][i](x[i])\n",
    "    return dataset.tree.predict(x)\n",
    "\n",
    "menuSelect = 0\n",
    "secondaryMenuSelect = 0\n",
    "\n",
    "while True:\n",
    "    if menuSelect == 0:     # Menu\n",
    "        secondaryMenuSelect = 0\n",
    "        print()\n",
    "        print(\"Select an Operation:\")\n",
    "        print(\"1.See Decision Tree\")\n",
    "        print(\"2.Test Decision Tree\")\n",
    "        print(\"3.Predict my own data with Tree\")\n",
    "        print(\"4.Exit\")\n",
    "        menuSelect = int(input())\n",
    "        if (menuSelect > 4 or menuSelect < 0):\n",
    "            menuSelect = 0\n",
    "            continue\n",
    "        continue\n",
    "    elif menuSelect == 1:   # Print a Decision Tree\n",
    "        printTreeOptions()\n",
    "        secondaryMenuSelect = int(input())\n",
    "        \n",
    "        if (secondaryMenuSelect > 4 or secondaryMenuSelect < 0):\n",
    "            secondaryMenuSelect = 0\n",
    "            continue\n",
    "        if secondaryMenuSelect == 0:\n",
    "            menuSelect = 0\n",
    "            continue\n",
    "        \n",
    "        if secondaryMenuSelect == 1:    # Iris\n",
    "            print(Iris.tree.print_tree())\n",
    "        elif secondaryMenuSelect == 2:  # Restaurant\n",
    "            print(Restaurant.tree.print_tree())\n",
    "        elif secondaryMenuSelect == 3:  # Connect4\n",
    "            print(Connect4.tree.print_tree())\n",
    "        else:                           # Weather\n",
    "            print(Weather.tree.print_tree())\n",
    "\n",
    "    elif menuSelect == 2:   # Test a Decision Tree using the testing data from the csv file\n",
    "\n",
    "        printTreeOptions()\n",
    "        secondaryMenuSelect = int(input())\n",
    "        \n",
    "        if (secondaryMenuSelect > 4 or secondaryMenuSelect < 0):\n",
    "            secondaryMenuSelect = 0\n",
    "            continue\n",
    "        if secondaryMenuSelect == 0:\n",
    "            menuSelect = 0\n",
    "            continue\n",
    "\n",
    "        if secondaryMenuSelect == 1:    # Iris\n",
    "            testTree(Iris)\n",
    "        elif secondaryMenuSelect == 2:  # Restaurant\n",
    "            testTree(Restaurant)\n",
    "        elif secondaryMenuSelect == 3:  # Connect4\n",
    "            testTree(Connect4)\n",
    "        else:                           # Weather\n",
    "            testTree(Weather)\n",
    "\n",
    "    elif menuSelect == 3:   # Insert data and get a prediction from one of the Decision Trees\n",
    "        printTreeOptions()\n",
    "        secondaryMenuSelect = int(input())\n",
    "        \n",
    "        if (secondaryMenuSelect > 4 or secondaryMenuSelect < 0):\n",
    "            secondaryMenuSelect = 0\n",
    "            continue\n",
    "        if secondaryMenuSelect == 0:\n",
    "            menuSelect = 0\n",
    "            continue\n",
    "        \n",
    "        if secondaryMenuSelect == 1:    # Iris\n",
    "            print(getAndPredictInput(Iris))\n",
    "        elif secondaryMenuSelect == 2:  # Restaurant\n",
    "            print(getAndPredictInput(Restaurant))\n",
    "        elif secondaryMenuSelect == 3:  # Connect4\n",
    "            print(getAndPredictInput(Connect4))\n",
    "        else:                           # Weather\n",
    "            print(getAndPredictInput(Weather))\n",
    "\n",
    "    elif menuSelect == 4:   # Exit the program\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd5b04",
   "metadata": {},
   "source": [
    "# **Conclusion**\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce240ff4",
   "metadata": {},
   "source": [
    "We created a Induced ID3 Decision Tree Algorithm that can classify both categorical and numerical values and predict a unlabeled instance of the dataset. We also made a class Data to pre-process all the data and do the intended splits. We obtained very good results, since all our right previsions are at least above 65%. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
